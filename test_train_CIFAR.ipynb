{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMzxydwndCIcABShhenHzAO",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/trousil8672/WENO/blob/main/test_train_CIFAR.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "PwjZgBfkeDVy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c782e5d6-3e4f-4e21-e91d-9de6a4ec93a0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tensorboardX\n",
            "  Downloading tensorboardX-2.6.2.2-py2.py3-none-any.whl (101 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/101.7 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━\u001b[0m \u001b[32m92.2/101.7 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m101.7/101.7 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from tensorboardX) (1.23.5)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorboardX) (23.2)\n",
            "Requirement already satisfied: protobuf>=3.20 in /usr/local/lib/python3.10/dist-packages (from tensorboardX) (3.20.3)\n",
            "Installing collected packages: tensorboardX\n",
            "Successfully installed tensorboardX-2.6.2.2\n"
          ]
        }
      ],
      "source": [
        "!pip install tensorboardX\n",
        "!pip install models\n",
        "\n",
        "import os\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "os.chdir('/content/drive/My Drive/')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Copy of train_CIFAR_BagDistillation_SharedEnc_Similarity_StuFilterSmoothed_DropPos.py\n",
        "import argparse\n",
        "import warnings\n",
        "import os\n",
        "import time\n",
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "import torch.optim\n",
        "import torch.nn as nn\n",
        "import torch.utils.data\n",
        "from tensorboardX import SummaryWriter\n",
        "# import models\n",
        "# from models.alexnet import alexnet_CIFAR10, alexnet_CIFAR10_Attention\n",
        "from models.alexnet import alexnet_CIFAR10_Encoder, teacher_Attention_head, student_head\n",
        "# from dataset_toy import Dataset_toy\n",
        "# from Datasets_loader.dataset_MNIST_challenge import MNIST_WholeSlide_challenge\n",
        "from Datasets_loader.dataset_MIL_CIFAR import CIFAR_WholeSlide_challenge\n",
        "import datetime\n",
        "import utliz\n",
        "import util\n",
        "import random\n",
        "from tqdm import tqdm\n",
        "import copy\n",
        "\n",
        "class Optimizer:\n",
        "    def __init__(self, model_encoder, model_teacherHead, model_studentHead,\n",
        "                 optimizer_encoder, optimizer_teacherHead, optimizer_studentHead,\n",
        "                 train_bagloader, train_instanceloader, test_bagloader, test_instanceloader,\n",
        "                 writer=None, num_epoch=100,\n",
        "                 dev=torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"),\n",
        "                 PLPostProcessMethod='NegGuide', StuFilterType='ReplaceAS', smoothE=100,\n",
        "                 stu_loss_weight_neg=0.1):\n",
        "        self.model_encoder = model_encoder\n",
        "        self.model_teacherHead = model_teacherHead\n",
        "        self.model_studentHead = model_studentHead\n",
        "        self.optimizer_encoder = optimizer_encoder\n",
        "        self.optimizer_teacherHead = optimizer_teacherHead\n",
        "        self.optimizer_studentHead = optimizer_studentHead\n",
        "        self.train_bagloader = train_bagloader\n",
        "        self.train_instanceloader = train_instanceloader\n",
        "        self.test_bagloader = test_bagloader\n",
        "        self.test_instanceloader = test_instanceloader\n",
        "        self.writer = writer\n",
        "        self.num_epoch = num_epoch\n",
        "        self.dev = dev\n",
        "        self.log_period = 10\n",
        "        self.PLPostProcessMethod = PLPostProcessMethod\n",
        "        self.StuFilterType = StuFilterType\n",
        "        self.smoothE = smoothE\n",
        "        self.stu_loss_weight_neg = stu_loss_weight_neg\n",
        "\n",
        "    def optimize(self):\n",
        "        self.Bank_all_Bags_label = None\n",
        "        self.Bank_all_instances_pred_byTeacher = None\n",
        "        self.Bank_all_instances_feat_byTeacher = None\n",
        "        self.Bank_all_instances_pred_processed = None\n",
        "\n",
        "        self.Bank_all_instances_pred_byStudent = None\n",
        "\n",
        "        # Load pre-extracted SimCLR features\n",
        "        # pre_trained_SimCLR_feat = self.train_instanceloader.dataset.ds_data_simCLR_feat[self.train_instanceloader.dataset.idx_all_slides].to(self.dev)\n",
        "        for epoch in range(self.num_epoch):\n",
        "            self.optimize_teacher(epoch)\n",
        "            self.evaluate_teacher(epoch)\n",
        "            # self.prediction_all_instance_teacher = self.infer_teacher_prediction_on_all_training_instances()\n",
        "            if epoch < 0:\n",
        "                pass\n",
        "            else:\n",
        "                self.optimize_student(epoch)\n",
        "                # self.Bank_all_instances_pred_processed = self.post_process_pred_byTeacher(self.Bank_all_instances_feat_byTeacher,\n",
        "                #                                                                           self.Bank_all_instances_pred_byTeacher,\n",
        "                #                                                                           self.Bank_all_Bags_label,\n",
        "                #                                                                           method=self.PLPostProcessMethod)\n",
        "                # # self.Bank_all_instances_pred_processed = self.post_process_pred_byTeacher(pre_trained_SimCLR_feat,\n",
        "                # #                                                                           self.Bank_all_instances_pred_byTeacher,\n",
        "                # #                                                                           self.Bank_all_Bags_label,\n",
        "                # #                                                                           method=self.PLPostProcessMethod)\n",
        "                #\n",
        "                # self.optimize_student_fromBank(epoch, self.Bank_all_instances_pred_processed)\n",
        "                self.evaluate_student(epoch)\n",
        "\n",
        "        return 0\n",
        "\n",
        "    def optimize_teacher(self, epoch):\n",
        "        self.model_encoder.train()\n",
        "        self.model_teacherHead.train()\n",
        "        self.model_studentHead.eval()\n",
        "        ## optimize teacher with bag-dataloader\n",
        "        # 1. change loader to bag-loader\n",
        "        loader = self.train_bagloader\n",
        "        # 2. optimize\n",
        "        patch_label_gt = torch.zeros([loader.dataset.__len__(), 100]).float().to(self.dev)  # only for patch-label available dataset\n",
        "        patch_label_pred = torch.zeros([loader.dataset.__len__(), 100]).float().to(self.dev)\n",
        "        bag_label_gt = torch.zeros([loader.dataset.__len__(), 1]).float().to(self.dev)\n",
        "        bag_label_pred = torch.zeros([loader.dataset.__len__(), 1]).float().to(self.dev)\n",
        "        feat_pred = torch.zeros([loader.dataset.__len__(), 100, 192*3*3]).float().to(self.dev)\n",
        "        for iter, (data, label, selected) in enumerate(tqdm(loader, desc='Teacher training')):\n",
        "            for i, j in enumerate(label):\n",
        "                if torch.is_tensor(j):\n",
        "                    label[i] = j.to(self.dev)\n",
        "            selected = selected.squeeze(0)\n",
        "            niter = epoch * len(loader) + iter\n",
        "\n",
        "            data = data.to(self.dev)\n",
        "            # bag_prediction, _, _, instance_attn_score = self.model_tea(data, returnBeforeSoftMaxA=True)\n",
        "            feat = self.model_encoder(data.squeeze(0))\n",
        "            if epoch > self.smoothE:\n",
        "                if \"FilterNegInstance\" in self.StuFilterType:\n",
        "                    # using student prediction to remove negative instance feat in the positive bag\n",
        "                    if label[1] == 1:\n",
        "                        with torch.no_grad():\n",
        "                            pred_byStudent = self.model_studentHead(feat)\n",
        "                            pred_byStudent = torch.softmax(pred_byStudent, dim=1)[:, 1]\n",
        "                        if '_Top' in self.StuFilterType:\n",
        "                            # strategy A: remove the topK most negative instance\n",
        "                            idx_to_keep = torch.topk(-pred_byStudent, k=int(self.StuFilterType.split('_Top')[-1]))[1]\n",
        "                        elif '_ThreProb' in self.StuFilterType:\n",
        "                            # strategy B: remove the negative instance above prob K\n",
        "                            idx_to_keep = torch.where(pred_byStudent >= int(self.StuFilterType.split('_Thre')[-1])/100.0)[0]\n",
        "                        feat_removedNeg = feat[idx_to_keep]\n",
        "                        bag_prediction, _, _, instance_attn_score = self.model_teacherHead(feat_removedNeg, returnBeforeSoftMaxA=True, scores_replaceAS=None)\n",
        "                        instance_attn_score = torch.cat([instance_attn_score, instance_attn_score.min()*torch.ones(1, 100-instance_attn_score.shape[1]).to(instance_attn_score.device)], dim=1)\n",
        "                        # with torch.no_grad():\n",
        "                        #     _, _, _, instance_attn_score = self.model_teacherHead(feat, returnBeforeSoftMaxA=True, scores_replaceAS=None)\n",
        "                    else:\n",
        "                        bag_prediction, _, _, instance_attn_score = self.model_teacherHead(feat, returnBeforeSoftMaxA=True, scores_replaceAS=None)\n",
        "                else:\n",
        "                    bag_prediction, _, _, instance_attn_score = self.model_teacherHead(feat, returnBeforeSoftMaxA=True, scores_replaceAS=None)\n",
        "            else:\n",
        "                bag_prediction, _, _, instance_attn_score = self.model_teacherHead(feat, returnBeforeSoftMaxA=True, scores_replaceAS=None)\n",
        "            bag_prediction = torch.softmax(bag_prediction, 1)\n",
        "            loss_teacher = -1. * (label[1] * torch.log(bag_prediction[0, 1]+1e-5) + (1. - label[1]) * torch.log(1. - bag_prediction[0, 1]+1e-5))\n",
        "            self.optimizer_encoder.zero_grad()\n",
        "            self.optimizer_teacherHead.zero_grad()\n",
        "            loss_teacher.backward()\n",
        "            self.optimizer_encoder.step()\n",
        "            self.optimizer_teacherHead.step()\n",
        "\n",
        "            feat_pred[selected, :, :] = feat.reshape(100, -1).detach()\n",
        "            patch_label_pred[selected, :] = instance_attn_score.detach().squeeze(0)\n",
        "            patch_label_gt[selected, :] = label[0]\n",
        "            bag_label_pred[selected] = bag_prediction.detach()[0, 1]\n",
        "            bag_label_gt[selected] = label[1]\n",
        "            if niter % self.log_period == 0:\n",
        "                self.writer.add_scalar('train_loss_Teacher', loss_teacher.item(), niter)\n",
        "\n",
        "        self.Bank_all_Bags_label = bag_label_gt\n",
        "        self.Bank_all_instances_pred_byTeacher = patch_label_pred\n",
        "        self.Bank_all_instances_feat_byTeacher = feat_pred\n",
        "        self.estimated_AttnScore_norm_para_min = patch_label_pred.min()\n",
        "        self.estimated_AttnScore_norm_para_max = patch_label_pred.max()\n",
        "        patch_label_pred_normed = self.norm_AttnScore2Prob(patch_label_pred)\n",
        "        instance_auc_ByTeacher = utliz.cal_auc(patch_label_gt.reshape(-1), patch_label_pred_normed.reshape(-1))\n",
        "\n",
        "        bag_auc_ByTeacher = utliz.cal_auc(bag_label_gt.reshape(-1), bag_label_pred.reshape(-1))\n",
        "        self.writer.add_scalar('train_instance_AUC_byTeacher', instance_auc_ByTeacher, epoch)\n",
        "        self.writer.add_scalar('train_bag_AUC_byTeacher', bag_auc_ByTeacher, epoch)\n",
        "        # print(\"Epoch:{} train_bag_AUC_byTeacher:{}\".format(epoch, bag_auc_ByTeacher))\n",
        "\n",
        "        ### more metric for teacher instance-level prediction (Pseudo-label)\n",
        "        # only statistics Pseudo-labels on Pos bag\n",
        "        patch_label_gt_PosBag = patch_label_gt[bag_label_gt[:, 0]==1, :].reshape(-1)\n",
        "        patch_label_pred_normed_PosBag = patch_label_pred_normed[bag_label_gt[:, 0]==1, :].reshape(-1).detach()\n",
        "        pseudo_label_metrics, pseudo_label_acc, pseudo_label_auc = utliz.cal_TPR_TNR_FPR_FNR(patch_label_gt_PosBag, patch_label_pred_normed_PosBag)\n",
        "\n",
        "        self.writer.add_scalar('train_pseudo_label_PosBag_TPR', pseudo_label_metrics[0], epoch)\n",
        "        self.writer.add_scalar('train_pseudo_label_PosBag_TNR', pseudo_label_metrics[1], epoch)\n",
        "        self.writer.add_scalar('train_pseudo_label_PosBag_FPR', pseudo_label_metrics[2], epoch)\n",
        "        self.writer.add_scalar('train_pseudo_label_PosBag_FNR', pseudo_label_metrics[3], epoch)\n",
        "        # self.writer.add_scalar('train_pseudo_label_PosBag_precision', pseudo_label_precision.item(), epoch)\n",
        "        self.writer.add_scalar('train_pseudo_label_PosBag_acc', pseudo_label_acc, epoch)\n",
        "        self.writer.add_scalar('train_pseudo_label_PosBag_auc', pseudo_label_auc, epoch)\n",
        "        return 0\n",
        "\n",
        "    def norm_AttnScore2Prob(self, attn_score):\n",
        "        prob = (attn_score - self.estimated_AttnScore_norm_para_min) / (self.estimated_AttnScore_norm_para_max - self.estimated_AttnScore_norm_para_min)\n",
        "        return prob\n",
        "\n",
        "    def post_process_pred_byTeacher(self, Bank_all_instances_feat, Bank_all_instances_pred, Bank_all_bags_label, method='NegGuide'):\n",
        "        if method=='NegGuide':\n",
        "            Bank_all_instances_pred_processed = Bank_all_instances_pred.clone()\n",
        "            Bank_all_instances_pred_processed = self.norm_AttnScore2Prob(Bank_all_instances_pred_processed).clamp(min=1e-5, max=1 - 1e-5)\n",
        "            idx_neg_bag = torch.where(Bank_all_bags_label[:, 0] == 0)[0]\n",
        "            Bank_all_instances_pred_processed[idx_neg_bag, :] = 0\n",
        "        elif method=='NegGuide_TopK':\n",
        "            Bank_all_instances_pred_processed = Bank_all_instances_pred.clone()\n",
        "            Bank_all_instances_pred_processed = self.norm_AttnScore2Prob(Bank_all_instances_pred_processed).clamp(min=1e-5, max=1 - 1e-5)\n",
        "            idx_pos_bag = torch.where(Bank_all_bags_label[:, 0] == 1)[0]\n",
        "            idx_neg_bag = torch.where(Bank_all_bags_label[:, 0] == 0)[0]\n",
        "            K = 3\n",
        "            idx_topK_inside_pos_bag = torch.topk(Bank_all_instances_pred_processed[idx_pos_bag, :], k=K, dim=-1, largest=True)[1]\n",
        "            Bank_all_instances_pred_processed[idx_pos_bag].scatter_(index=idx_topK_inside_pos_bag, dim=1, value=1)\n",
        "            Bank_all_instances_pred_processed[idx_neg_bag, :] = 0\n",
        "        elif method=='NegGuide_Similarity':\n",
        "            Bank_all_instances_pred_processed = Bank_all_instances_pred.clone()\n",
        "            Bank_all_instances_pred_processed = self.norm_AttnScore2Prob(Bank_all_instances_pred_processed).clamp(min=1e-5, max=1 - 1e-5)\n",
        "            idx_pos_bag = torch.where(Bank_all_bags_label[:, 0] == 1)[0]\n",
        "            idx_neg_bag = torch.where(Bank_all_bags_label[:, 0] == 0)[0]\n",
        "            K = 1\n",
        "            idx_topK_inside_pos_bag = torch.topk(Bank_all_instances_pred_processed[idx_pos_bag, :], k=K, dim=-1, largest=True)[1]\n",
        "            Bank_all_instances_pred_processed[idx_pos_bag].scatter_(index=idx_topK_inside_pos_bag, dim=1, value=1)\n",
        "            Bank_all_Pos_instances_feat = Bank_all_instances_feat[idx_pos_bag]\n",
        "            Bank_mostSalient_Pos_instances_feat = []\n",
        "            for i in range(Bank_all_Pos_instances_feat.shape[0]):\n",
        "                Bank_mostSalient_Pos_instances_feat.append(Bank_all_Pos_instances_feat[i, idx_topK_inside_pos_bag[i, 0], :].unsqueeze(0).unsqueeze(0))\n",
        "            Bank_mostSalient_Pos_instances_feat = torch.cat(Bank_mostSalient_Pos_instances_feat, dim=0)\n",
        "\n",
        "            distance_matrix = Bank_all_Pos_instances_feat - Bank_mostSalient_Pos_instances_feat\n",
        "            distance_matrix = torch.norm(distance_matrix, dim=-1, p=2)\n",
        "            Bank_all_instances_pred_processed[idx_pos_bag, :] = self.distanceMatrix2PL(distance_matrix)\n",
        "            Bank_all_instances_pred_processed[idx_neg_bag, :] = 0\n",
        "        else:\n",
        "            raise TypeError\n",
        "        return Bank_all_instances_pred_processed\n",
        "\n",
        "    def distanceMatrix2PL(self, distance_matrix, method='percentage'):\n",
        "        # distance_matrix is of shape NxL (Num of Positive Bag * Bag Length)\n",
        "        # represents the distance between each instance with their corresponding most salient instance\n",
        "        # return Pseudo-labels of shape NxL (value should belong to [0,1])\n",
        "\n",
        "        if method == 'softMax':\n",
        "            # 1. just use softMax to keep PLs value fall into [0,1]\n",
        "            similarity_matrix = 1/(distance_matrix + 1e-5)\n",
        "            pseudo_labels = torch.softmax(similarity_matrix, dim=1)\n",
        "        elif method == 'percentage':\n",
        "            # 2. use percentage to keep n% PL=1, 1-n% PL=0\n",
        "            p = 0.1  # 10% is set\n",
        "            threshold_v = distance_matrix.topk(k=int(100 * p), dim=1)[0][:, -1].unsqueeze(1).repeat([1, 100])  # of size Nx100\n",
        "            pseudo_labels = torch.zeros_like(distance_matrix)\n",
        "            pseudo_labels[distance_matrix >= threshold_v] = 0.0\n",
        "            pseudo_labels[distance_matrix < threshold_v] = 1.0\n",
        "        elif method == 'threshold':\n",
        "            # 3. use threshold to set PLs of instance with distance above the threshold to 1\n",
        "            raise TypeError\n",
        "        else:\n",
        "            raise TypeError\n",
        "\n",
        "        ## visulaize the pseudo_labels distribution of inside each bag\n",
        "        # import matplotlib.pyplot as plt\n",
        "        # plt.figure()\n",
        "        # plt.hist(pseudo_labels.cpu().numpy().reshape(-1))\n",
        "\n",
        "        return pseudo_labels\n",
        "\n",
        "    def optimize_student(self, epoch):\n",
        "        self.model_teacherHead.train()\n",
        "        self.model_encoder.train()\n",
        "        self.model_studentHead.train()\n",
        "        ## optimize teacher with instance-dataloader\n",
        "        # 1. change loader to instance-loader\n",
        "        loader = self.train_instanceloader\n",
        "        # 2. optimize\n",
        "        patch_label_gt = torch.zeros([loader.dataset.__len__(), 1]).long().to(self.dev)  # only for patch-label available dataset\n",
        "        patch_label_pred = torch.zeros([loader.dataset.__len__(), 1]).float().to(self.dev)\n",
        "        bag_label_gt = torch.zeros([loader.dataset.__len__(), 1]).long().to(self.dev)\n",
        "        patch_corresponding_slide_idx = torch.zeros([loader.dataset.__len__(), 1]).long().to(self.dev)\n",
        "        for iter, (data, label, selected) in enumerate(tqdm(loader, desc='Student training')):\n",
        "            for i, j in enumerate(label):\n",
        "                if torch.is_tensor(j):\n",
        "                    label[i] = j.to(self.dev)\n",
        "            selected = selected.squeeze(0)\n",
        "            niter = epoch * len(loader) + iter\n",
        "\n",
        "            data = data.to(self.dev)\n",
        "\n",
        "            # get teacher output of instance\n",
        "            feat = self.model_encoder(data)\n",
        "            with torch.no_grad():\n",
        "                _, _, _, instance_attn_score = self.model_teacherHead(feat, returnBeforeSoftMaxA=True)\n",
        "                pseudo_instance_label = self.norm_AttnScore2Prob(instance_attn_score).clamp(min=1e-5, max=1-1e-5).squeeze(0)\n",
        "                # set true negative patch label to [1, 0]\n",
        "                pseudo_instance_label[label[1] == 0] = 0\n",
        "            # # DEBUG: Assign GT patch label\n",
        "            # pseudo_instance_label = label[0]\n",
        "            # get student output of instance\n",
        "            patch_prediction = self.model_studentHead(feat)\n",
        "            patch_prediction = torch.softmax(patch_prediction, dim=1)\n",
        "\n",
        "            # cal loss\n",
        "            loss_student = -1. * torch.mean(self.stu_loss_weight_neg * (1-pseudo_instance_label) * torch.log(patch_prediction[:, 0] + 1e-5) +\n",
        "                                            (1-self.stu_loss_weight_neg) * pseudo_instance_label * torch.log(patch_prediction[:, 1] + 1e-5))\n",
        "            self.optimizer_encoder.zero_grad()\n",
        "            self.optimizer_studentHead.zero_grad()\n",
        "            loss_student.backward()\n",
        "            self.optimizer_encoder.step()\n",
        "            self.optimizer_studentHead.step()\n",
        "\n",
        "            patch_corresponding_slide_idx[selected, 0] = label[2]\n",
        "            patch_label_pred[selected, 0] = patch_prediction.detach()[:, 1]\n",
        "            patch_label_gt[selected, 0] = label[0]\n",
        "            bag_label_gt[selected, 0] = label[1]\n",
        "            if niter % self.log_period == 0:\n",
        "                self.writer.add_scalar('train_loss_Student', loss_student.item(), niter)\n",
        "\n",
        "        instance_auc_ByStudent = utliz.cal_auc(patch_label_gt.reshape(-1), patch_label_pred.reshape(-1))\n",
        "        self.writer.add_scalar('train_instance_AUC_byStudent', instance_auc_ByStudent, epoch)\n",
        "        # print(\"Epoch:{} train_instance_AUC_byStudent:{}\".format(epoch, instance_auc_ByStudent))\n",
        "\n",
        "        # cal bag-level auc\n",
        "        bag_label_gt_coarse = []\n",
        "        bag_label_prediction = []\n",
        "        available_bag_idx = patch_corresponding_slide_idx.unique()\n",
        "        for bag_idx_i in available_bag_idx:\n",
        "            idx_same_bag_i = torch.where(patch_corresponding_slide_idx == bag_idx_i)\n",
        "            if bag_label_gt[idx_same_bag_i].max() != bag_label_gt[idx_same_bag_i].max():\n",
        "                raise\n",
        "            bag_label_gt_coarse.append(bag_label_gt[idx_same_bag_i].max())\n",
        "            bag_label_prediction.append(patch_label_pred[idx_same_bag_i].max())\n",
        "        bag_label_gt_coarse = torch.tensor(bag_label_gt_coarse)\n",
        "        bag_label_prediction = torch.tensor(bag_label_prediction)\n",
        "        bag_auc_ByStudent = utliz.cal_auc(bag_label_gt_coarse.reshape(-1), bag_label_prediction.reshape(-1))\n",
        "        self.writer.add_scalar('train_bag_AUC_byStudent', bag_auc_ByStudent, epoch)\n",
        "        return 0\n",
        "\n",
        "    def optimize_student_fromBank(self, epoch, Bank_all_instances_pred):\n",
        "        self.model_teacherHead.train()\n",
        "        self.model_encoder.train()\n",
        "        self.model_studentHead.train()\n",
        "        ## optimize teacher with instance-dataloader\n",
        "        # 1. change loader to instance-loader\n",
        "        loader = self.train_instanceloader\n",
        "        # 2. optimize\n",
        "        patch_label_gt = torch.zeros([loader.dataset.__len__(), 1]).long().to(self.dev)  # only for patch-label available dataset\n",
        "        patch_label_pred = torch.zeros([loader.dataset.__len__(), 1]).float().to(self.dev)\n",
        "        bag_label_gt = torch.zeros([loader.dataset.__len__(), 1]).long().to(self.dev)\n",
        "        patch_corresponding_slide_idx = torch.zeros([loader.dataset.__len__(), 1]).long().to(self.dev)\n",
        "        for iter, (data, label, selected) in enumerate(tqdm(loader, desc='Student training')):\n",
        "            for i, j in enumerate(label):\n",
        "                if torch.is_tensor(j):\n",
        "                    label[i] = j.to(self.dev)\n",
        "            selected = selected.squeeze(0)\n",
        "            niter = epoch * len(loader) + iter\n",
        "\n",
        "            data = data.to(self.dev)\n",
        "\n",
        "            # get teacher output of instance\n",
        "            feat = self.model_encoder(data)\n",
        "            # with torch.no_grad():\n",
        "            #     _, _, _, instance_attn_score = self.model_teacherHead(feat, returnBeforeSoftMaxA=True)\n",
        "            #     pseudo_instance_label = self.norm_AttnScore2Prob(instance_attn_score).clamp(min=1e-5, max=1-1e-5).squeeze(0)\n",
        "            #     # set true negative patch label to [1, 0]\n",
        "            #     pseudo_instance_label[label[1] == 0] = 0\n",
        "\n",
        "            pseudo_instance_label = Bank_all_instances_pred[selected//100, selected%100]\n",
        "            # # DEBUG: Assign GT patch label\n",
        "            # pseudo_instance_label = label[0]\n",
        "            # get student output of instance\n",
        "            patch_prediction = self.model_studentHead(feat)\n",
        "            patch_prediction = torch.softmax(patch_prediction, dim=1)\n",
        "\n",
        "            # cal loss\n",
        "            loss_student = -1. * torch.mean(0.1 * (1-pseudo_instance_label) * torch.log(patch_prediction[:, 0] + 1e-5) +\n",
        "                                            0.9 * pseudo_instance_label * torch.log(patch_prediction[:, 1] + 1e-5))\n",
        "            self.optimizer_encoder.zero_grad()\n",
        "            self.optimizer_studentHead.zero_grad()\n",
        "            loss_student.backward()\n",
        "            self.optimizer_encoder.step()\n",
        "            self.optimizer_studentHead.step()\n",
        "\n",
        "            patch_corresponding_slide_idx[selected, 0] = label[2]\n",
        "            patch_label_pred[selected, 0] = patch_prediction.detach()[:, 1]\n",
        "            patch_label_gt[selected, 0] = label[0]\n",
        "            bag_label_gt[selected, 0] = label[1]\n",
        "            if niter % self.log_period == 0:\n",
        "                self.writer.add_scalar('train_loss_Student', loss_student.item(), niter)\n",
        "\n",
        "        self.Bank_all_instances_pred_byStudent = patch_label_pred\n",
        "        instance_auc_ByStudent = utliz.cal_auc(patch_label_gt.reshape(-1), patch_label_pred.reshape(-1))\n",
        "        bag_auc_ByStudent = 0\n",
        "        self.writer.add_scalar('train_instance_AUC_byStudent', instance_auc_ByStudent, epoch)\n",
        "        self.writer.add_scalar('train_bag_AUC_byStudent', bag_auc_ByStudent, epoch)\n",
        "        # print(\"Epoch:{} train_instance_AUC_byStudent:{}\".format(epoch, instance_auc_ByStudent))\n",
        "        return 0\n",
        "\n",
        "    def evaluate(self, epoch, loader, log_name_prefix=''):\n",
        "        return 0\n",
        "\n",
        "    def evaluate_teacher(self, epoch):\n",
        "        self.model_encoder.eval()\n",
        "        self.model_teacherHead.eval()\n",
        "        self.model_studentHead.eval()\n",
        "        ## optimize teacher with bag-dataloader\n",
        "        # 1. change loader to bag-loader\n",
        "        loader = self.test_bagloader\n",
        "        # 2. optimize\n",
        "        patch_label_gt = torch.zeros([loader.dataset.__len__(), 100]).float().to(self.dev)  # only for patch-label available dataset\n",
        "        patch_label_pred = torch.zeros([loader.dataset.__len__(), 100]).float().to(self.dev)\n",
        "        bag_label_gt = torch.zeros([loader.dataset.__len__(), 1]).float().to(self.dev)\n",
        "        bag_label_prediction_withAttnScore = torch.zeros([loader.dataset.__len__(), 1]).float().to(self.dev)\n",
        "        bag_label_prediction_withStudentPred = torch.zeros([loader.dataset.__len__(), 1]).float().to(self.dev)\n",
        "        for iter, (data, label, selected) in enumerate(tqdm(loader, desc='Teacher evaluating')):\n",
        "            for i, j in enumerate(label):\n",
        "                if torch.is_tensor(j):\n",
        "                    label[i] = j.to(self.dev)\n",
        "            selected = selected.squeeze(0)\n",
        "            niter = epoch * len(loader) + iter\n",
        "\n",
        "            data = data.to(self.dev)\n",
        "            with torch.no_grad():\n",
        "                feat = self.model_encoder(data.squeeze(0))\n",
        "                ## In evaluation: replace Attention Scores with student prediction\n",
        "                # bag_prediction, _, _, instance_attn_score = self.model_teacherHead(feat, returnBeforeSoftMaxA=True, scores_replaceAS=None)\n",
        "                # bag_prediction = torch.softmax(bag_prediction, 1)\n",
        "\n",
        "                patch_prediction_byStudent = self.model_studentHead(feat)[:, 1].unsqueeze(0)\n",
        "                bag_prediction_withAttnScore, _, _, instance_attn_score = self.model_teacherHead(feat, returnBeforeSoftMaxA=True, scores_replaceAS=None)\n",
        "                bag_prediction_withStudentPred, _, _, _ = self.model_teacherHead(feat, returnBeforeSoftMaxA=True, scores_replaceAS=patch_prediction_byStudent)\n",
        "                bag_prediction_withAttnScore = torch.softmax(bag_prediction_withAttnScore, 1)\n",
        "                bag_prediction_withStudentPred = torch.softmax(bag_prediction_withStudentPred, 1)\n",
        "\n",
        "            patch_label_pred[selected, :] = instance_attn_score.detach().squeeze(0)\n",
        "            patch_label_gt[selected, :] = label[0]\n",
        "            bag_label_prediction_withAttnScore[selected] = bag_prediction_withAttnScore.detach()[0, 1]\n",
        "            bag_label_prediction_withStudentPred[selected] = bag_prediction_withStudentPred.detach()[0, 1]\n",
        "            bag_label_gt[selected] = label[1]\n",
        "\n",
        "        patch_label_pred_normed = (patch_label_pred - patch_label_pred.min()) / (patch_label_pred.max() - patch_label_pred.min())\n",
        "        instance_auc_ByTeacher = utliz.cal_auc(patch_label_gt.reshape(-1), patch_label_pred_normed.reshape(-1))\n",
        "        bag_auc_ByTeacher_withAttnScore = utliz.cal_auc(bag_label_gt.reshape(-1), bag_label_prediction_withAttnScore.reshape(-1))\n",
        "        bag_auc_ByTeacher_withStudentPred = utliz.cal_auc(bag_label_gt.reshape(-1), bag_label_prediction_withStudentPred.reshape(-1))\n",
        "        self.writer.add_scalar('test_instance_AUC_byTeacher', instance_auc_ByTeacher, epoch)\n",
        "        self.writer.add_scalar('test_bag_AUC_byTeacher', bag_auc_ByTeacher_withAttnScore, epoch)\n",
        "        self.writer.add_scalar('test_bag_AUC_byTeacher_withStudentPred', bag_auc_ByTeacher_withStudentPred, epoch)\n",
        "        return 0\n",
        "\n",
        "    def evaluate_student(self, epoch):\n",
        "        self.model_encoder.eval()\n",
        "        self.model_studentHead.eval()\n",
        "        ## optimize teacher with instance-dataloader\n",
        "        # 1. change loader to instance-loader\n",
        "        loader = self.test_instanceloader\n",
        "        # 2. optimize\n",
        "        patch_label_gt = torch.zeros([loader.dataset.__len__(), 1]).long().to(self.dev)  # only for patch-label available dataset\n",
        "        patch_label_pred = torch.zeros([loader.dataset.__len__(), 1]).float().to(self.dev)\n",
        "        bag_label_gt = torch.zeros([loader.dataset.__len__(), 1]).long().to(self.dev)\n",
        "        patch_corresponding_slide_idx = torch.zeros([loader.dataset.__len__(), 1]).long().to(self.dev)\n",
        "        for iter, (data, label, selected) in enumerate(tqdm(loader, desc='Student evaluating')):\n",
        "            for i, j in enumerate(label):\n",
        "                if torch.is_tensor(j):\n",
        "                    label[i] = j.to(self.dev)\n",
        "            selected = selected.squeeze(0)\n",
        "            niter = epoch * len(loader) + iter\n",
        "\n",
        "            data = data.to(self.dev)\n",
        "\n",
        "            # get student output of instance\n",
        "            with torch.no_grad():\n",
        "                feat = self.model_encoder(data)\n",
        "                patch_prediction = self.model_studentHead(feat)\n",
        "                patch_prediction = torch.softmax(patch_prediction, dim=1)\n",
        "\n",
        "            patch_corresponding_slide_idx[selected, 0] = label[2]\n",
        "            patch_label_pred[selected, 0] = patch_prediction.detach()[:, 1]\n",
        "            patch_label_gt[selected, 0] = label[0]\n",
        "            bag_label_gt[selected, 0] = label[1]\n",
        "\n",
        "        instance_auc_ByStudent = utliz.cal_auc(patch_label_gt.reshape(-1), patch_label_pred.reshape(-1))\n",
        "        self.writer.add_scalar('test_instance_AUC_byStudent', instance_auc_ByStudent, epoch)\n",
        "        # print(\"Epoch:{} test_instance_AUC_byStudent:{}\".format(epoch, instance_auc_ByStudent))\n",
        "\n",
        "        # cal bag-level auc\n",
        "        bag_label_gt_coarse = []\n",
        "        bag_label_prediction = []\n",
        "        available_bag_idx = patch_corresponding_slide_idx.unique()\n",
        "        for bag_idx_i in available_bag_idx:\n",
        "            idx_same_bag_i = torch.where(patch_corresponding_slide_idx == bag_idx_i)\n",
        "            if bag_label_gt[idx_same_bag_i].max() != bag_label_gt[idx_same_bag_i].max():\n",
        "                raise\n",
        "            bag_label_gt_coarse.append(bag_label_gt[idx_same_bag_i].max())\n",
        "            bag_label_prediction.append(patch_label_pred[idx_same_bag_i].max())\n",
        "        bag_label_gt_coarse = torch.tensor(bag_label_gt_coarse)\n",
        "        bag_label_prediction = torch.tensor(bag_label_prediction)\n",
        "        bag_auc_ByStudent = utliz.cal_auc(bag_label_gt_coarse.reshape(-1), bag_label_prediction.reshape(-1))\n",
        "        self.writer.add_scalar('test_bag_AUC_byStudent', bag_auc_ByStudent, epoch)\n",
        "        return 0\n",
        "\n",
        "\n",
        "def str2bool(v):\n",
        "    \"\"\"\n",
        "    Input:\n",
        "        v - string\n",
        "    output:\n",
        "        True/False\n",
        "    \"\"\"\n",
        "    if isinstance(v, bool):\n",
        "       return v\n",
        "    if v.lower() in ('yes', 'true', 't', 'y', '1'):\n",
        "        return True\n",
        "    elif v.lower() in ('no', 'false', 'f', 'n', '0'):\n",
        "        return False\n",
        "    else:\n",
        "        raise argparse.ArgumentTypeError('Boolean value expected.')\n",
        "\n",
        "\n",
        "def get_parser():\n",
        "    parser = argparse.ArgumentParser(description='PyTorch Implementation of Self-Label')\n",
        "    # optimizer\n",
        "    parser.add_argument('--epochs', default=1500, type=int, help='number of epochs')\n",
        "    parser.add_argument('--batch_size', default=256, type=int, help='batch size (default: 256)')\n",
        "    parser.add_argument('--lr', default=0.001, type=float, help='initial learning rate (default: 0.05)')\n",
        "    parser.add_argument('--lrdrop', default=1500, type=int, help='multiply LR by 0.5 every (default: 150 epochs)')\n",
        "    parser.add_argument('--wd', default=-5, type=float, help='weight decay pow (default: (-5)')\n",
        "    parser.add_argument('--dtype', default='f64', choices=['f64', 'f32'], type=str, help='SK-algo dtype (default: f64)')\n",
        "\n",
        "    # SK algo\n",
        "    parser.add_argument('--nopts', default=100, type=int, help='number of pseudo-opts (default: 100)')\n",
        "    parser.add_argument('--augs', default=3, type=int, help='augmentation level (default: 3)')\n",
        "    parser.add_argument('--lamb', default=25, type=int, help='for pseudoopt: lambda (default:25) ')\n",
        "\n",
        "    # architecture\n",
        "    # parser.add_argument('--arch', default='alexnet_MNIST', type=str, help='alexnet or resnet (default: alexnet)')\n",
        "\n",
        "    # housekeeping\n",
        "    parser.add_argument('--device', default='0', type=str, help='GPU devices to use for storage and model')\n",
        "    parser.add_argument('--modeldevice', default='0', type=str, help='GPU numbers on which the CNN runs')\n",
        "    parser.add_argument('--exp', default='self-label-default', help='path to experiment directory')\n",
        "    parser.add_argument('--workers', default=0, type=int,help='number workers (default: 6)')\n",
        "    parser.add_argument('--comment', default='DEBUG', type=str, help='name for tensorboardX')\n",
        "    parser.add_argument('--log-intv', default=1, type=int, help='save stuff every x epochs (default: 1)')\n",
        "    parser.add_argument('--log_iter', default=200, type=int, help='log every x-th batch (default: 200)')\n",
        "    parser.add_argument('--seed', default=10, type=int, help='random seed')\n",
        "\n",
        "    parser.add_argument('--pos_patch_ratio', default=0.2, type=float, help='positive patch ratio in positive slide')\n",
        "    parser.add_argument('--bag_length', default=100, type=int, help='bag length, when MIL_SeLA used, set to 600 corresponding to a whole slide')\n",
        "\n",
        "    parser.add_argument('--PLPostProcessMethod', default='NegGuide', type=str,\n",
        "                        help='Post-processing method of Attention Scores to build Pseudo Lables',\n",
        "                        choices=['NegGuide', 'NegGuide_TopK', 'NegGuide_Similarity'])\n",
        "    parser.add_argument('--StuFilterType', default='FilterNegInstance_Top100', type=str,\n",
        "                        help='Type of using Student Prediction to imporve Teacher [ReplaceAS, FilterNegInstance_Top1]')\n",
        "    parser.add_argument('--smoothE', default=9999, type=int, help='num of epoch to apply StuFilter')\n",
        "    parser.add_argument('--stu_loss_weight_neg', default=0.2, type=float, help='weight of neg instances in stu training')\n",
        "    return parser.parse_args()\n",
        "\n"
      ],
      "metadata": {
        "id": "aTyDRD7GfpgX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    args = get_parser()\n",
        "\n",
        "    # torch.manual_seed(args.seed)\n",
        "    # random.seed(args.seed)\n",
        "    # np.random.seed(args.seed)\n",
        "    name = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")+\"_%s\" % args.comment.replace('/', '_') + \\\n",
        "           \"_Seed{}_Bs{}\".format( args.seed, args.batch_size)\n",
        "\n",
        "    # name = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")+\"_%s\" % args.comment.replace('/', '_') + \\\n",
        "    #        \"_Seed{}_Bs{}_lr{}_BagLength{}_PosPatchRatio{}_PLPostProcessBy{}_StuFilterType{}_smoothE{}_weightN{}\".format(\n",
        "    #            args.seed, args.batch_size, args.lr, args.bag_length, args.pos_patch_ratio,\n",
        "    #            args.PLPostProcessMethod, args.StuFilterType, args.smoothE, args.stu_loss_weight_neg)\n",
        "    try:\n",
        "        args.device = [int(item) for item in args.device.split(',')]\n",
        "    except AttributeError:\n",
        "        args.device = [int(args.device)]\n",
        "    args.modeldevice = args.device\n",
        "    util.setup_runtime(seed=42, cuda_dev_id=list(np.unique(args.modeldevice + args.device)))\n",
        "\n",
        "    print(name, flush=True)\n",
        "\n",
        "    writer = SummaryWriter('./runs_CIFAR/%s'%name)\n",
        "    writer.add_text('args', \" \\n\".join(['%s %s' % (arg, getattr(args, arg)) for arg in vars(args)]))\n",
        "\n",
        "    # Setup model\n",
        "    model_encoder = alexnet_CIFAR10_Encoder().to('cuda:0')\n",
        "    model_teacherHead = teacher_Attention_head().to('cuda:0')\n",
        "    model_studentHead = student_head().to('cuda:0')\n",
        "\n",
        "    optimizer_encoder = torch.optim.SGD(model_encoder.parameters(), lr=args.lr)\n",
        "    optimizer_teacherHead = torch.optim.SGD(model_teacherHead.parameters(), lr=args.lr)\n",
        "    optimizer_studentHead = torch.optim.SGD(model_studentHead.parameters(), lr=args.lr)\n",
        "\n",
        "    # Setup loaders\n",
        "    positive_num = [9]\n",
        "    negative_num = [0, 1, 2, 3, 4, 5, 6, 7, 8]\n",
        "    train_ds_return_instance = CIFAR_WholeSlide_challenge(train=True, positive_num=positive_num, negative_num=negative_num,\n",
        "                                          bag_length=args.bag_length, return_bag=False,\n",
        "                                          num_img_per_slide=args.bag_length, pos_patch_ratio=args.pos_patch_ratio, pos_slide_ratio=0.5, transform=None)\n",
        "    train_ds_return_bag = copy.deepcopy(train_ds_return_instance)\n",
        "    train_ds_return_bag.return_bag = True\n",
        "    val_ds_return_instance = CIFAR_WholeSlide_challenge(train=False, positive_num=positive_num, negative_num=negative_num,\n",
        "                                        bag_length=args.bag_length, return_bag=False,\n",
        "                                        num_img_per_slide=args.bag_length, pos_patch_ratio=args.pos_patch_ratio, pos_slide_ratio=0.5, transform=None)\n",
        "    val_ds_return_bag = CIFAR_WholeSlide_challenge(train=False, positive_num=positive_num, negative_num=negative_num,\n",
        "                                        bag_length=args.bag_length, return_bag=True,\n",
        "                                        num_img_per_slide=args.bag_length, pos_patch_ratio=args.pos_patch_ratio, pos_slide_ratio=0.5, transform=None)\n",
        "\n",
        "    train_loader_instance = torch.utils.data.DataLoader(train_ds_return_instance, batch_size=args.batch_size, shuffle=True, num_workers=args.workers, drop_last=False)\n",
        "    train_loader_bag = torch.utils.data.DataLoader(train_ds_return_bag, batch_size=1, shuffle=True, num_workers=args.workers, drop_last=False)\n",
        "    val_loader_instance = torch.utils.data.DataLoader(val_ds_return_instance, batch_size=args.batch_size, shuffle=False, num_workers=args.workers, drop_last=False)\n",
        "    val_loader_bag = torch.utils.data.DataLoader(val_ds_return_bag, batch_size=1, shuffle=False, num_workers=args.workers, drop_last=False)\n",
        "\n",
        "    print(\"[Data] {} training samples\".format(len(train_loader_instance.dataset)))\n",
        "    print(\"[Data] {} evaluating samples\".format(len(val_loader_instance.dataset)))\n",
        "\n",
        "    if torch.cuda.device_count() > 1:\n",
        "        print(\"Let's use\", len(args.modeldevice), \"GPUs for the model\")\n",
        "        if len(args.modeldevice) == 1:\n",
        "            print('single GPU model', flush=True)\n",
        "        else:\n",
        "            model_encoder = nn.DataParallel(model_encoder, device_ids=list(range(len(args.modeldevice))))\n",
        "            model_teacherHead = nn.DataParallel(model_teacherHead, device_ids=list(range(len(args.modeldevice))))\n",
        "            optimizer_studentHead = nn.DataParallel(optimizer_studentHead, device_ids=list(range(len(args.modeldevice))))\n",
        "\n",
        "    # Setup optimizer\n",
        "    o = Optimizer(model_encoder=model_encoder, model_teacherHead=model_teacherHead, model_studentHead=model_studentHead,\n",
        "                  optimizer_encoder=optimizer_encoder, optimizer_teacherHead=optimizer_teacherHead, optimizer_studentHead=optimizer_studentHead,\n",
        "                  train_bagloader=train_loader_bag, train_instanceloader=train_loader_instance,\n",
        "                  test_bagloader=val_loader_bag, test_instanceloader=val_loader_instance,\n",
        "                  writer=writer, num_epoch=args.epochs,\n",
        "                  dev=torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"),\n",
        "                  PLPostProcessMethod=args.PLPostProcessMethod, StuFilterType=args.StuFilterType, smoothE=args.smoothE,\n",
        "                  stu_loss_weight_neg=args.stu_loss_weight_neg)\n",
        "    # Optimize\n",
        "    o.optimize()"
      ],
      "metadata": {
        "id": "GTeOT5Urfuvy"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}